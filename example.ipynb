{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preprocessing\n",
    "\n",
    "The method requires a corpus to train a custom model of word embeddings. In this example, the corpus is a collection of political speeches from the Canadian House of Commons. I use data from 1980 to 2015, which should be large enough to fit a reliable word embeddings model. (You may find the full Canadian corpus here: www.lipad.ca)\n",
    "\n",
    "You may download the data here: https://drive.google.com/uc?id=1u-lnejm4bzDm7t3YulSowzLaS26IHLpz\n",
    "\n",
    "To replicate the approach used in the study, I will lemmatize the corpus and detect the parts of speech for each word. \n",
    "\n",
    "Not so many researchers get into the trouble of lemmatization and POS tagging before fitting embeddings. The rationale was that lemmatization reduces the size of the vocabulary, and thus should improve the accuracy of word vectors. As for parts of speech, the rationale is that they should help to reduce ambiguity, by distinguishing between usages of words with multiple meanings. Since the categories for parts of speech are a bit too specific for this task, I convert the Penn Treebank POS Tags into a simplified format that identifies nouns, adjectives, verbs and adverbs. This will allow to filter the vocabulary for words of substantive interest later on. Alternatively, one could choose to skip these preprocessing steps. \n",
    "\n",
    "The following script relies on a Python wrapper to call the Stanford CoreNLP library. It will output a copy of our corpus, but containing lemmas and their associated part of speech.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================#\n",
    "# Downloading Stanford CoreNLP.\n",
    "#==========================================================================#\n",
    "# The exclamation mark indicates that we call the command from our operating system.\n",
    "! wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================#\n",
    "# Unzipping the file.\n",
    "#==========================================================================#\n",
    "! unzip stanford-corenlp-latest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP # Will require installing.\n",
    "import json\n",
    "\n",
    "#==========================================================================#\n",
    "# Loading CoreNLP wrapper.\n",
    "#==========================================================================#\n",
    "\n",
    "nlp = StanfordCoreNLP('stanford-corenlp-4.4.0/') # Make sure the numbers correspond to your version.\n",
    "props={'annotators': 'tokenize,lemma,pos', 'pipelineLanguage':'en'}\n",
    "\n",
    "#==========================================================================#\n",
    "# A dictionary to zero in on parts of speech of interest:\n",
    "#==========================================================================#\n",
    "posmap = {'JJ':'a', 'JJR':'a', 'JJS':'a', # Adjectives\n",
    "        'NN':'n', 'NNS':'n', # Nouns, excluding proper nouns\n",
    "        'RB':'r', 'RBR':'r', 'RBS':'r', # Adverbs\n",
    "        'UH':'u', # Interjections\n",
    "        'VB':'v', 'VBD':'v', 'VBG':'v', 'VBN':'v', 'VBP':'v', 'VBZ':'v'} # Verbs\n",
    "\n",
    "#==========================================================================#\n",
    "# A function to call CoreNLP and retrieve words converted to 'lemmas_POS' format.\n",
    "#==========================================================================#\n",
    "def lemmatize(text):\n",
    "    res = json.loads(nlp.annotate(text, properties=props))\n",
    "    lemmas = [token['lemma'] + '_' + posmap.get(token['pos'],'') for s in res['sentences'] for token in s['tokens']]\n",
    "    lemmas = [l.lower() for l in lemmas if ' ' not in l and l.count('_')==1] # This will exclude malformed tokens.\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "#==========================================================================#\n",
    "# Our main loop, to read the corpus and transform it.\n",
    "#==========================================================================#\n",
    "progress = 0\n",
    "with open('lipad8015_lemmas.csv', 'w') as fout: # Saving a copy of the output.\n",
    "    with open('lipad8015.csv', 'r') as fin: # The original corpus.\n",
    "        for line in fin: # Stream lines to save memory.\n",
    "            newline = lemmatize(line) # Process the line.\n",
    "            fout.write(newline + '\\n') # Save processed line to file.\n",
    "            progress += 1\n",
    "            if progress%10000==0:\n",
    "                print(f\"Completed {progress} lines.\")           \n",
    "\n",
    "#==========================================================================#\n",
    "# Closing connection to CoreNLP\n",
    "#==========================================================================#\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also consider alternatives like SpaCy, stanza, and other NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fitting custom word embeddings model\n",
    "\n",
    "Next, I will use the GloVe program to fit word embeddings. Many alternatives are available, like the word2vec model or embeddings from large scale language models. \n",
    "\n",
    "GloVe is written in C, a \"compiled language.\" It will require compiling on your local machine.\n",
    "\n",
    "Once the program is compiled, we can run it on our custom corpus and generate embeddings. \n",
    "\n",
    "The file fit.sh is a shell script to call GloVe. Parameters of the models can be modified at will. For this example, let us fit a model with a dimension of 300 and a window of 15 words, similar to that used in the original study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================#\n",
    "# Downloading GloVe.\n",
    "#==========================================================================#\n",
    "# The study used the original code (Glove 1.0), but since a new one is available, let us use the latest one.\n",
    "! wget https://nlp.stanford.edu/software/GloVe-1.2.zip\n",
    "! unzip GloVe-1.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================#\n",
    "# Compiling GloVe.\n",
    "#==========================================================================#\n",
    "# We need to compile the C program first.\n",
    "! cd GloVe-1.2 && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================#\n",
    "# Computing word embeddings\n",
    "#==========================================================================#\n",
    "# Let us call a shell script that will run the GloVe program as desired, using our new corpus. \n",
    "# We can call this script from here, after making it executable:\n",
    "! chmod +x fit.sh\n",
    "! ./fit.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands above assume a Mac or Linux operating system. You may need to edit slightly for use on a Windows OS. The parameters for GloVe are left at default values, but you could fit multiple models, as desired, by editing the fit.sh file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generating an anxiety lexicon\n",
    "\n",
    "The final step is to generate a lexicon for anxiety. We start from seed words that I had curated manually, and identifying the poles of an axis ranging from confidence to anxiety.  \n",
    "\n",
    "The objective is to rank the remaining lemmas from our vocabulary based on where they fall on a scale from confidence to anxiety. For that, we can calculate the similarity of each lemma's embedding to those of the lemmas in each group of seeds. The method is inspired from Peter Turney's work for lexicon creation. We introduced an adaptation to word embeddings in more details in a separate study (https://github.com/lrheault/emotion). \n",
    "\n",
    "The commands below will create our final lexicon. Optionally, we can filter the lexicon to retain only words in our main categories for parts of speech. The logic is that this will exclude words like proper nouns, which we do not expect to represent indicators of anxiety in text. (Although depending on use cases, you may have a justification to consider proper nouns.) Another option is to rescale the scores between -1 and 1 (from confidence to anxiety), so that it is simpler to interpret, which is done here.\n",
    "\n",
    "The resulting dictionary will not be identical to the one used in the original study, but should be similar and just as useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "\n",
    "#==========================================================================#\n",
    "# GloVe files have no header, so we must use that option to load our model:\n",
    "#==========================================================================#\n",
    "model = KeyedVectors.load_word2vec_format('lipad-vectors-300d.txt', no_header=True) \n",
    "\n",
    "#==========================================================================#\n",
    "# Compute cosine similarity of a word against the two groups of seed words:\n",
    "#==========================================================================#\n",
    "def create_score(word, positive_seeds, negative_seeds):\n",
    "    positive = sum([model.similarity(word, x) for x in positive_seeds])/len(positive_seeds) \n",
    "    negative = sum([model.similarity(word, x) for x in negative_seeds])/len(negative_seeds) \n",
    "    return (positive - negative)\n",
    "\n",
    "# A function to rescale/normalize a pandas series.\n",
    "def rescale(X, newmin=-1, newmax=1):\n",
    "    return [(newmax-newmin)*((x-X.min())/(X.max()-X.min()))+newmin for x in X]\n",
    "\n",
    "#==========================================================================#\n",
    "# Loading manually curated seed lemmas for anxiety:\n",
    "#==========================================================================#\n",
    "seeds = pd.read_csv('seeds.csv')\n",
    "positive_seeds = [lemma + '_' + pos for lemma,pos,score in zip(seeds.lemma, seeds.pos, seeds.score) if score==1] # Anxiety words \n",
    "negative_seeds = [lemma + '_' + pos for lemma,pos,score in zip(seeds.lemma, seeds.pos, seeds.score) if score==-1] # Confidence words\n",
    "\n",
    "#==========================================================================#\n",
    "# Loading vocabulary.\n",
    "#==========================================================================#\n",
    "vocab = []\n",
    "with open('lipad-vocab.txt','r') as fin:\n",
    "    for line in fin:\n",
    "        word_freq = line.split()\n",
    "        if len(word_freq)==2: # Eliminates malformed tokens.\n",
    "            word,freq = word_freq\n",
    "            if int(freq)>=200: # Filter on frequency as desired.\n",
    "                vocab.append((word,freq))\n",
    "vocab = [(word,freq) for word,freq in vocab if word not in positive_seeds and word not in negative_seeds]\n",
    "print(f'Processing a total of {len(vocab)} words.')\n",
    "    \n",
    "#==========================================================================#\n",
    "# Calculating scores for each word:\n",
    "#==========================================================================#\n",
    "progress = 0\n",
    "anxiety_lexicon = []\n",
    "for word, freq in vocab:\n",
    "    lemma_pos = word.split('_')\n",
    "    if len(lemma_pos)==2 and lemma_pos[0].isalpha():\n",
    "        lemma, pos = lemma_pos\n",
    "        score = create_score(word, positive_seeds, negative_seeds)\n",
    "        anxiety_lexicon.append((lemma, pos, score))\n",
    "    progress += 1\n",
    "    if progress%1000==0:\n",
    "        print(f'Completed {progress} words.')\n",
    "        \n",
    "#==========================================================================#\n",
    "# Storing results in a pandas data frame.\n",
    "#==========================================================================#\n",
    "anxiety_lexicon = pd.DataFrame(anxiety_lexicon, columns=['lemma', 'pos', 'score'])\n",
    "# We can rescale the scores between -1 and 1 \n",
    "anxiety_lexicon['score'] = rescale(anxiety_lexicon.score)\n",
    "\n",
    "# We can now implement additional filters on the lexicon. \n",
    "# For instance, we can choose to keep only the main POS categories identified earlier.\n",
    "anxiety_lexicon = anxiety_lexicon[pd.notnull(anxiety_lexicon.pos) & (anxiety_lexicon.pos!='')]\n",
    "anxiety_lexicon.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "\n",
    "# We can attach the original seeds to the lexicon, with the respective bounding values.\n",
    "anxiety_lexicon = pd.concat([anxiety_lexicon,seeds])\n",
    "\n",
    "#==========================================================================#\n",
    "# Saving our lexicon:\n",
    "#==========================================================================#\n",
    "print(f'The created lexicon contains {anxiety_lexicon.shape[0]} words.')\n",
    "anxiety_lexicon.to_csv('new_anxiety_lexicon.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
